{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03-2-Simple Neural Network.ipynb의 사본의 사본","provenance":[{"file_id":"1oyFaaosSycXKWfzx6jirfxGMUefh4Inx","timestamp":1616553449648}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"l5fhg52fGtBW"},"source":["data.py model.py train.py evaluate.py"]},{"cell_type":"markdown","metadata":{"id":"hCRVCE--NqDn"},"source":["# Simple Neural Network\n","\n","- PyTorch를 활용한 간단한 인공신경망 모델 만들고 학습하기\n","- 데이터셋은 보스턴 집값 데이터셋을 활용한다 ([링크](https://www.kaggle.com/vikrishnan/boston-house-prices))\n","- sklearn.datasets에서는 다양한 데이터들을 제공한다.\n","([링크](https://scikit-learn.org/stable/datasets/index.html))"]},{"cell_type":"code","metadata":{"id":"CUXQy6m4NinT"},"source":["from sklearn.datasets import load_boston\n","import pandas as pd \n","import numpy as np\n","from sklearn.model_selection import train_test_split \n","from sklearn.preprocessing import MinMaxScaler\n","\n","import torch\n","from torch import nn, optim \n","from torch.utils.data import DataLoader, Dataset \n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pBUmJr0-OCrN"},"source":["## 1. 데이터"]},{"cell_type":"code","metadata":{"id":"_ZQmCaQNOCGO"},"source":["bos = load_boston() # 데이터 불러오기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TuX5gJqdOGkH"},"source":["# data, feature name 등 다양한 정보를 포함하고 있다.\n","df = pd.DataFrame(bos.data) \n","df.columns = bos.feature_names \n","df['Price'] = bos.target \n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlrQI4rzOHiw"},"source":["# 데이터를 numpy 배열로 만들기\n","X = df.drop('Price', axis=1).to_numpy() \n","Y = df['Price'].to_numpy().reshape((-1,1))\n","\n","# 데이터 스케일링\n","scaler = MinMaxScaler() \n","scaler.fit(X) \n","X = scaler.transform(X)\n","\n","scaler.fit(Y)\n","Y = scaler.transform(Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBecoIokONeZ"},"source":["# 텐서 데이터로 변환하는 클래스\n","class TensorData(Dataset):\n","    def __init__(self, x_data, y_data):\n","        self.x_data = torch.FloatTensor(x_data)\n","        self.y_data = torch.FloatTensor(y_data)\n","        self.len = self.y_data.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.x_data[index], self.y_data[index] \n","\n","    def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzVhUXVPOboy"},"source":["# 전체 데이터를 학습 데이터와 평가 데이터로 나눈다.\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n","\n","# 학습 데이터, 시험 데이터 배치 형태로 구축하기\n","trainsets = TensorData(X_train, Y_train)\n","trainloader = torch.utils.data.DataLoader(trainsets, batch_size=16, shuffle=True)\n","\n","testsets = TensorData(X_test, Y_test)\n","testloader = torch.utils.data.DataLoader(testsets, batch_size=16, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j0hK-omgOgC1"},"source":["## 2. 모델"]},{"cell_type":"code","metadata":{"id":"yt5oslwdOdLB"},"source":["class SimpleNN(nn.Module):\n","    def __init__(self):\n","        super().__init__() # 모델 연산 정의\n","        self.fc1 = nn.Linear(13, 50, bias=True) # 입력층(13) -> 은닉층1(50)으로 가는 연산\n","        self.fc2 = nn.Linear(50, 30, bias=True) # 은닉층1(50) -> 은닉층2(30)으로 가는 연산\n","        self.fc3 = nn.Linear(30, 1, bias=True) # 은닉층2(30) -> 출력층(1)으로 가는 연산\n","        # 드랍아웃은 과적합(overfitting)을 방지하기 위해 노드의 일부를 배제하고 계산하는 방식이기 때문에 절대로 출력층에 사용하지 않는다\n","        self.dropout = nn.Dropout(0.2) # Dropout 비율을 20%로 설정한다\n","\n","    def forward(self, x): # 모델 연산의 순서를 정의\n","        x = F.relu(self.fc1(x)) # Linear 계산 후 활성화 함수 ReLU를 적용한다.  \n","        x = self.dropout(F.relu(self.fc2(x))) # 은닉층2에서 드랍아웃을 적용한다.(즉, 30개의 20%인 6개의 노드가 계산에서 제외된다.)\n","        x = F.relu(self.fc3(x)) # Linear 계산 후 활성화 함수 ReLU를 적용한다.  \n","      \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cOsM8rhOk31"},"source":["model = SimpleNN()\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"baLspa2KOyls"},"source":["## 3. 학습"]},{"cell_type":"code","metadata":{"id":"YxNBwThBOvsK"},"source":["loss_ = []\n","num_epoch = 100\n","n = len(trainloader)\n","\n","for epoch in range(num_epoch):\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0): # 무작위로 섞인 16개 데이터가 있는 배치가 하나 씩 들어온다.\n","\n","        inputs, values = data # data에는 X, Y가 들어있다.\n","\n","        optimizer.zero_grad() # 최적화 초기화\n","        \n","        outputs = model(inputs) # 모델에 입력값 대입 후 예측값 산출\n","        loss = criterion(outputs, values) # 손실 함수 계산\n","        loss.backward() # 손실 함수 기준으로 역전파 설정 \n","        optimizer.step() # 역전파를 진행하고 가중치 업데이트\n","        \n","        running_loss += loss.item() # epoch 마다 평균 loss를 계산하기 위해 배치 loss를 더한다.\n"," \n","\n","    loss_.append(running_loss/n) # MSE(Mean Squared Error) 계산\n","\n","        \n","print('Finished Training')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lby3oPVSOx12"},"source":["import matplotlib.pyplot as plt\n","plt.plot(loss_)\n","plt.title(\"Training Loss\")\n","plt.xlabel(\"epoch\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gFqXvTs9O_PM"},"source":["## 4. 평가"]},{"cell_type":"code","metadata":{"id":"k5TiDtGLO310"},"source":["def evaluation(dataloader):\n","    \n","    predictions = torch.tensor([], dtype=torch.float) # 예측값을 저장하는 텐서\n","    actual = torch.tensor([], dtype=torch.float) # 실제값을 저장하는 텐서\n","        \n","    with torch.no_grad():\n","        model.eval() # 평가를 할 때에는 .eval() 반드시 사용해야 한다.\n","        for data in dataloader:\n","            inputs, values = data\n","            outputs = model(inputs)\n","\n","            predictions = torch.cat((predictions, outputs), 0) # cat을 통해 예측값을 누적\n","            actual = torch.cat((actual, values), 0) # cat을 통해 실제값을 누적\n","    \n","    predictions = predictions.numpy()\n","    actual = actual.numpy()\n","    rmse = np.sqrt(mean_squared_error(predictions, actual))\n","    \n","    return rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6B3ULmJpPBZm"},"source":["from sklearn.metrics import mean_squared_error\n","\n","train_rmse = evaluation(trainloader)\n","test_rmse = evaluation(testloader)\n","\n","print(\"Train RMSE: \",train_rmse)\n","print(\"Test RMSE: \",test_rmse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l6ZTT8FETutH"},"source":["- (실습)\n","  - 이번에는 위의 내용을 바탕으로 iris dataset으로 실습하기\n","  - [링크](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) 참고하여 데이터셋 다운로드\n","  - 아래 모델 구조와 loss를 활용해서 구현하기"]},{"cell_type":"code","metadata":{"id":"NtxdnA7ZPDxz"},"source":["class SimpleClassifier(nn.Module):\n","    def __init__(self):\n","        super(SimpleClassifier, self).__init__()\n","        self.fc1 = nn.Linear(4, 100)\n","        self.fc2 = nn.Linear(100, 100)\n","        self.fc3 = nn.Linear(100, 3)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, X):\n","        X = F.relu(self.fc1(X))\n","        X = self.fc2(X)\n","        X = self.fc3(X)\n","        X = self.softmax(X)\n","\n","        return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcW1yL5CU6Rt"},"source":["criterion = nn.CrossEntropyLoss() # cross entropy loss"],"execution_count":null,"outputs":[]}]}