{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-Fine-Tuning BERT for Sentence Classification.ipynb","provenance":[{"file_id":"https://github.com/YuvalPeleg/transformers-workshop/blob/master/Fine_Tuning_Sentence_Classification.ipynb","timestamp":1616691233269}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RX_ZDhicpHkV"},"source":["## Install and Import"]},{"cell_type":"code","metadata":{"id":"0NmMdkZO8R6q"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ok002ceNB8E7"},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertConfig, BertForSequenceClassification\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","import random, time, datetime\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYsV4H8fCpZ-"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(torch.cuda.get_device_name(0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"guw6ZNtaswKc"},"source":["## Load Dataset\n","\n","- Use [IMDB movie review](http://ai.stanford.edu/~amaas/data/sentiment/) dataset. \n","- This is a dataset for binary sentiment classification task."]},{"cell_type":"code","metadata":{"id":"HeZvu4KuVGmV"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFOMsPAPVPcz"},"source":["import os\n","path = \"/content/drive/MyDrive/3.AI_트랜스포머_이재원 강사님(3.24~3.26)/data\"\n","\n","train_df = pd.read_csv(os.path.join(path, 'IMDb_train_data.csv'))[:100]\n","test_df = pd.read_csv(os.path.join(path, 'IMDb_test_data.csv'))[:20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9O9F7yfTVeTT"},"source":["train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GuE5BqICAne2"},"source":["# Create sentence and label lists\n","sentences = train_df.review.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n","sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n","labels = train_df.sentiment.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ex5O1eV-Pfct"},"source":["## Inputs"]},{"cell_type":"markdown","metadata":{"id":"BTREubVNFiz4"},"source":["Next, import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary."]},{"cell_type":"code","metadata":{"id":"7pSv1kuerunY"},"source":["# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n","# In the original paper, the authors used a length of 512.\n","MAX_LEN = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z474sSC6oe7A"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def tokenize_and_trim(sent):\n","  tokens = tokenizer.tokenize(sent)\n","  return tokens[:MAX_LEN - 1] + [tokens[-1]]\n","\n","tokenized_texts = [tokenize_and_trim(sent) for sent in sentences]\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"87_kXUeT2-br"},"source":["BERT requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n","\n","- **input ids**: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n","- **segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n","- **attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n","- **labels**: a single value of 1 or 0. In our task 1 means \"grammatical\" and 0 means \"ungrammatical\""]},{"cell_type":"code","metadata":{"id":"mFhowDMohU4H"},"source":["# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDs-MYtYH8sL"},"source":["# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KhGulL1pExCT"},"source":["Create attention masks "]},{"cell_type":"code","metadata":{"id":"cDoC24LeEv3N"},"source":["# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFbE-UHvsb7-"},"source":["# Use train_test_split to split our data into train and validation sets for training\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2018, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jw5K2A5Ko1RF"},"source":["# Convert all of our data into torch tensors, the required datatype for our model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEgLpFVlo1Z-"},"source":["# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n","batch_size = 16\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pNl8khAhPYju"},"source":["## Train Model"]},{"cell_type":"markdown","metadata":{"id":"WnQW9E-bBCRt"},"source":["Load BERT! \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n","\n","<img src='http://jalammar.github.io/images/bert-classifier.png' width=700px>\n","\n","source: [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/)"]},{"cell_type":"code","metadata":{"id":"gFsCTp_mporB"},"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","model = model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8o-VEBobKwHk"},"source":["Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n","\n","For the purposes of fine-tuning, the authors recommend the following hyperparameter ranges:\n","- Batch size: 16, 32\n","- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n","- Number of epochs: 2, 3, 4"]},{"cell_type":"code","metadata":{"id":"QxSMw0FrptiL"},"source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLs72DuMODJO"},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 1\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 학습률을 조금씩 감소시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBBzYC7jX5uA"},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6U9i-v1YFVh"},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cUUirhfXCrI"},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑(https://sanghyu.tistory.com/87)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QXZhFb4LnV5"},"source":["Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. At each pass we need to:\n","\n","Training loop:\n","- Tell the model to compute gradients by setting the model in train mode\n","- Unpack our data inputs and labels\n","- Load data onto the GPU for acceleration\n","- Clear out the gradients calculated in the previous pass. In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n","- Forward pass (feed input data through the network)\n","- Backward pass (backpropagation)\n","- Tell the network to update parameters with optimizer.step()\n","- Track variables for monitoring progress\n","\n","Evalution loop:\n","- Tell the model not to compute gradients by setting th emodel in evaluation mode\n","- Unpack our data inputs and labels\n","- Load data onto the GPU for acceleration\n","- Forward pass (feed input data through the network)\n","- Compute loss on our validation data and track variables for monitoring progress\n","\n","So please read carefully through the comments to get an understanding of what's happening. If you're unfamiliar with pytorch a quick look at some of their [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) will help show you that training loops really involve only a few simple steps; the rest is usually just decoration and logging.  "]},{"cell_type":"code","metadata":{"id":"2namviD6YWao"},"source":[""],"execution_count":null,"outputs":[]}]}