{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06-Seq2Seq, Attention, Transformer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6plTVCIA_1b1"},"source":["1. Seq2Seq \n","  - https://medium.com/platfarm/어텐션-메커니즘과-transfomer-self-attention-842498fd3225\n","  - https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html\n","2. Transformer\n","  - https://tutorials.pytorch.kr/beginner/transformer_tutorial.html"]},{"cell_type":"code","metadata":{"id":"7VTX-AmM_x6B"},"source":[""],"execution_count":null,"outputs":[]}]}