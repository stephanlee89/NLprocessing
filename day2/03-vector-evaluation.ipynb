{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03-vector-evaluation.ipynb","provenance":[],"collapsed_sections":["MhxMBJTdIRD_"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DXnukir-Dt4t"},"source":["# Evaluate Vector Representation\n","- word vector가 아닌 sentence vector로 뉴스기사의 유사도를 비교합니다\n","\n","1. 'news_sample_test.txt' 파일을 load한다. 이 때, 개별 뉴스기사의 첫번째 문장만 활용한다. 예를 들어, 1번 뉴스기사에 41개 문장이 있다면, 그 중 첫번째 문장만 선택한다.\n","2.   word2vec과 fastText 모델을 load한다\n","3. word vector로 sentence vector를 만든다.\n","  1. 문장을 단어(또는 토큰)단위로 나눈다.\n","  2. 단어의 embedding vector를 word2vec과 fastText 모델에서 불러온다.\n","  3. **문장의 모든 단어의 embedding vector값을 전부 합친다.**\n","  4. 합쳐준 값을 문장벡터로 간주한다.\n","4. 이제 각 뉴스기사의 요약의 embedding vector를 활용해서, 개별 뉴스기사가 서로 유사한지 비교한다(cosine similarity 사용)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"6gRWg0IAGO9M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616640641295,"user_tz":-540,"elapsed":25548,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"}},"outputId":"ce2e069a-5f20-448c-d131-dcf0172ec838"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YVXZTkYSGSCQ"},"source":["import os\n","path = '/content/drive/MyDrive/'\n","filename = 'news_sample_test.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MhxMBJTdIRD_"},"source":["### 전처리"]},{"cell_type":"code","metadata":{"id":"C3T54aThISk1"},"source":["!pip install hanja\n","# 형태소 기반 토크나이징 (Konlpy)\n","!python3 -m pip install konlpy\n","# mecab (ubuntu, mac 기준)\n","# 다른 os 설치 방법 및 자세한 내용은 다음 참고: https://konlpy.org/ko/latest/install/#id1\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","\n","import re, hanja\n","from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n","removal_list =  \"‘, ’, ◇, ‘, ”,  ’, ', ·, \\“, ·, △, ●,  , ■, (, ), \\\", >>, `, /, -,∼,=,ㆍ<,>, .,?, !,【,】, …, ◆,%\"\n","\n","EMAIL_PATTERN = re.compile(r'''(([a-zA-Z0-9._%+-]+)@([a-zA-Z0-9.-]+)(\\.[a-zA-Z]{2,4}))''', re.VERBOSE)\n","URL_PATTERN = re.compile(\"(ftp|http|https)?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", re.VERBOSE)\n","MULTIPLE_SPACES = re.compile(' +', re.UNICODE)\n","\n","def get_tokenizer(tokenizer_name):\n","    if tokenizer_name == \"komoran\":\n","        tokenizer = Komoran()\n","    elif tokenizer_name == \"okt\":\n","        tokenizer = Okt()\n","    elif tokenizer_name == \"mecab\":\n","        tokenizer = Mecab()\n","    elif tokenizer_name == \"hannanum\":\n","        tokenizer = Hannanum()\n","    else:\n","        tokenizer = Kkma()\n","        \n","    return tokenizer\n","\n","def tokenize(tokenizer_name, original_sent, pos=False):\n","    tokenizer = get_tokenizer(tokenizer_name)\n","    sentence = original_sent.replace('\\n', '').strip()\n","    if pos:\n","        tokens = tokenizer.pos(sentence)\n","        tokens = [morph + \"/\" + tag for morph, tag in tokens]\n","    else:\n","      # tokenizer.nouns(sentence) -> 명사만 추출\n","        tokens = tokenizer.morphs(sentence)\n","        \n","    # tokenized_sent = ' '.join(post_process(tokens))\n","    tokenized_sent = ' '.join(tokens)\n","    \n","    return tokenized_sent\n","\n","def cleansing_other(sentence: str = None) -> str:\n","    \"\"\"\n","    문장을 전처리 (이메일, URL, 공백 등 제거) 하는 함수\n","    :param sentence: 전처리 대상 문장\n","    :return: 전처리 완료된 문장\n","    \"\"\"\n","    sentence = re.sub(EMAIL_PATTERN, ' ', sentence)\n","    sentence = re.sub(URL_PATTERN, ' ', sentence)\n","    sentence = re.sub(MULTIPLE_SPACES, ' ', sentence)\n","    sentence = sentence.replace(\", )\", \"\")\n","    \n","    return sentence\n","\n","def cleansing_chinese(sentence: str = None) -> str:\n","    \"\"\"\n","    한자를 변환하는 전처리를 하는 함수\n","    :param sentence: 전처리 대상 문장\n","    :return: 전처리 완료된 문장\n","    \"\"\"\n","    # chinese character를 앞뒤로 괄호가 감싸고 있을 경우, 대부분 한글 번역임\n","    sentence = re.sub(\"\\([\\u2E80-\\u2FD5\\u3190-\\u319f\\u3400-\\u4DBF\\u4E00-\\u9FCC\\uF900-\\uFAAD]+\\)\", \"\", sentence)\n","    # 다른 한자가 있다면 한글로 치환\n","    if re.search(\"[\\u2E80-\\u2FD5\\u3190-\\u319f\\u3400-\\u4DBF\\u4E00-\\u9FCC\\uF900-\\uFAAD]\", sentence) is not None:\n","        sentence = hanja.translate(sentence, 'substitution')\n","\n","    return sentence\n","\n","def cleansing_special(sentence: str = None) -> str:\n","    \"\"\"\n","    특수문자를 전처리를 하는 함수\n","    :param sentence: 전처리 대상 문장\n","    :return: 전처리 완료된 문장\n","    \"\"\"\n","    sentence = re.sub(\"[.,\\'\\\"’‘”“!?]\", \"\", sentence)\n","    sentence = re.sub(\"[^가-힣0-9a-zA-Z\\\\s]\", \" \", sentence)\n","    sentence = re.sub(\"\\s+\", \" \", sentence)\n","    \n","    sentence = sentence.translate(str.maketrans(removal_list, ' '*len(removal_list)))\n","    sentence = sentence.strip()\n","    \n","    return sentence\n","\n","def cleansing_numbers(sentence: str = None) -> str:\n","    \"\"\"\n","    숫자를 전처리(delexicalization) 하는 함수\n","    :param sentence: 전처리 대상 문장\n","    :return: 전처리 완료된 문장\n","    \"\"\"\n","    \n","    sentence = re.sub('[0-9]+', 'NUM', sentence)\n","    sentence = re.sub('NUM\\s+', \"NUM\", sentence)\n","    sentence = re.sub('[NUM]+', \"NUM\", sentence)\n","    \n","    return sentence\n","\n","def preprocess_sent(sentence: str = None) -> str:\n","    \"\"\"\n","    모든 전처리를 수행 하는 함수\n","    :param sentence: 전처리 대상 문장\n","    :return: 전처리 완료된 문장\n","    \"\"\"\n","    sent_clean = sentence\n","    sent_clean = cleansing_other(sent_clean)\n","    sent_clean = cleansing_chinese(sent_clean)\n","    sent_clean = cleansing_special(sent_clean)\n","    sent_clean = cleansing_numbers(sent_clean)\n","    sent_clean = re.sub('\\s+', ' ', sent_clean)\n","    sent_clean = tokenize('mecab', sent_clean)\n","\n","    return sent_clean"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74UcJxDhGggW"},"source":["## 1번\n","1. 'news_sample.txt' 파일을 load한다.\n","2. 이 때, 개별 뉴스기사의 첫 3개 문장을 활용한다. 예를 들어, 1번 뉴스기사에 41개 문장이 있다면, 그 중 1번부터 3번까지만 선택한다.\n","  - 이를 위해 `kss` 를 설치해야한다\n","3. 3개 문장을 전부 하나로 연결한다. 이 때, 띄어쓰기(' ')를 활용한다. 이를 뉴스의 요약이라고 간주한다\n","4. 각 뉴스기사의 요약을 전처리한다\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QyrCzvTVGttB"},"source":["!pip install kss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"px4fmO5_Gsc0"},"source":["import kss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXmwqjgNGaUu"},"source":["news_summ = []\n","with open(os.path.join(path, filename), 'r', encoding='utf-8') as f:\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uwfliux7Hza3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cU7NMIFJJBuH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fnNnpiPJGJOo"},"source":["## 2번\n","\n","-   word2vec과 fastText 모델을 load한다\n"]},{"cell_type":"code","metadata":{"id":"g9r1jS5WGF0R"},"source":["from gensim.models import KeyedVectors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FjohhJ7VDrPp"},"source":["w2v_model = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VwMKPCOFGEPj"},"source":["ft_model = "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fqy1EwyyJk4I"},"source":["## 3번\n","- word vector로 sentence vector를 만든다.\n","  1. 문장을 단어(또는 토큰)단위로 나눈다.\n","  2. 단어의 embedding vector를 word2vec과 fastText 모델에서 불러온다.\n","  3. **문장의 모든 단어의 embedding vector값을 전부 합친다.**\n","  4. 합쳐준 값을 문장벡터로 간주한다. 합쳐진 벡터를 summ_vec라고 한다"]},{"cell_type":"code","metadata":{"id":"fSAcCn2hSc9w"},"source":["# 안녕 하 세요. -> (1,300), (1,300), (1,300) -> [3, 300]\n","# sentence = '안녕 하 세요' -> for loop -> 안녕 -> model['안녕'] -> 300차원 벡터 -> 하 model['하'] -> 300차원 벡터\n","# 300차원벡터를 append를 한다 \n","\n","'''\n","sent_vec_list = \n","[[a1, ..., a300],\n","[b1, ..., b300],\n","[c1, ..., c300]]\n","\n","sent_vec =\n","[a1+b1+c1, ... , a300+b300+c300]\n","\n","'''\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6JMoEiALYyP"},"source":["import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XzEuVXisSSYH"},"source":["def get_sent_vector(model, sentence):\n","  sent_vec = []\n","  for word in sentence.split(' '):\n","    # out of vocabulary = oov \n","    if word in model.wv.vocab.keys():\n","      sent_vec.append(model[word])\n","    # fastText\n","    # word: 써비스 -> model.vocab -> string distance -> 서비스\n","  return sent_vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTv2fA6KJy2O"},"source":["# model.wv.vocab.keys()\n","# model['word'(변수명)]\n","def get_sent_vector(model, sentence):\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"osVUG3GWKqTf"},"source":["# summ_vec를 만든다\n","# np.summ(sent_vec, axis=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3DR6QW0qLD3f"},"source":["df = pd.DataFrame(data=results, columns=['news_id','summ_vectors'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPudrQBcMnZ5"},"source":["## 4번\n","\n","- 이제 각 뉴스기사의 요약의 embedding vector를 활용해서, 개별 뉴스기사가 서로 유사한지 비교한다(cosine similarity 사용)\n","- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"]},{"cell_type":"code","metadata":{"id":"DZRrSow5MdAz"},"source":["from sklearn.metrics.pairwise import cosine_similarity\n"],"execution_count":null,"outputs":[]}]}