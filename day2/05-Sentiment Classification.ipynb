{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"05-Sentiment Classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"I4H72nPYqAwV"},"source":["import os, random\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torchtext import data, datasets\n","from torchtext.legacy.data import BucketIterator\n","from torchtext.legacy import data\n","from torchtext.legacy.data import TabularDataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R0Yg9b8XqJtl","executionInfo":{"elapsed":20319,"status":"ok","timestamp":1616656591696,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"4e7d4f55-87f0-4051-f4f6-8b6999ca66b9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = \"/content/drive/MyDrive/Transformer/data\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OZoEN1JIrE5d"},"source":["## 1. Load Dataset and Split"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"RMze1J1kqZZ8","executionInfo":{"elapsed":2866,"status":"ok","timestamp":1616656598636,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"d81b3ac3-a254-4a55-97e6-9899cf23daed"},"source":["df = pd.read_csv(os.path.join(path,'IMDb_Reviews.csv'))\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>My family and I normally do not watch local mo...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Believe it or not, this was at one time the wo...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>After some internet surfing, I found the \"Home...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>One of the most unheralded great works of anim...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>It was the Sixties, and anyone with long hair ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  sentiment\n","0  My family and I normally do not watch local mo...          1\n","1  Believe it or not, this was at one time the wo...          0\n","2  After some internet surfing, I found the \"Home...          0\n","3  One of the most unheralded great works of anim...          1\n","4  It was the Sixties, and anyone with long hair ...          0"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yk47P55KqdDX","executionInfo":{"elapsed":552,"status":"ok","timestamp":1616656601379,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"3d807939-3474-48e8-f7f9-b8c25ceb886f"},"source":["print(len(df))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["50000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tQnHl6OMqhKK"},"source":["train_df = df[:int(len(df)*0.8)]\n","test_df = df[int(len(df)*0.8):]\n","\n","train_df.to_csv(os.path.join(path, \"IMDb_train_data.csv\"), index=False)\n","test_df.to_csv(os.path.join(path, \"IMDb_test_data.csv\"), index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"880tm-Mzq6Wz"},"source":["## 2. Define TorchText Field "]},{"cell_type":"markdown","metadata":{"id":"T48wpK7pszkC"},"source":["- sequential : 시퀀스 데이터 여부. (True가 기본값)\n","- use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값)\n","- tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)\n","- lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)\n","batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부. (False가 기본값)\n","- is_target : 레이블 데이터 여부. (False가 기본값)\n","- fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다.\n","- include_lengths : 토크나이즈된 문장의 길이를 같이 return 한다(True 또는 False)"]},{"cell_type":"code","metadata":{"id":"wQi4GInqq03e"},"source":["# 필드 정의\n","TEXT = data.Field(sequential=True,\n","                  use_vocab=True,\n","                  tokenize=lambda x: x.split(),\n","                  lower=True,\n","                  batch_first=True,\n","                  include_lengths=True)\n","\n","LABEL = data.LabelField(dtype = torch.float)\n","\n","# LABEL = data.Field(sequential=False,\n","#                    use_vocab=False,\n","#                    batch_first=False,\n","#                    is_target=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UID43-Tvs4Wn"},"source":["## 3. Create Dataset"]},{"cell_type":"code","metadata":{"id":"UKR3b166s7f6"},"source":["train_data, test_data = TabularDataset.splits(\n","    path=path,\n","    train='IMDb_train_data.csv', test='IMDb_test_data.csv', format='csv',\n","    fields=[('text', TEXT), ('label', LABEL)], skip_header=True)\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(1234))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIxmrHdMtCno","executionInfo":{"elapsed":4715,"status":"ok","timestamp":1616656623059,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"caf910f7-e476-4a8b-c40f-e3743fd48c3a"},"source":["print(vars(train_data[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'text': ['this', 'is', 'the', 'finest', 'film', 'ever', 'made', 'to', 'deal', 'with', 'the', 'subject', 'of', 'aids.', \"it's\", 'a', 'documentary', 'about', 'two', 'men', 'living', 'with', 'and', 'dying', 'of', 'this', 'illness.', 'the', 'film', 'is', 'beautiful,', 'heartbreaking,', 'funny,', 'and', 'incredibly', 'moving.', 'above', 'all,', 'it', 'is', 'an', 'amazing', 'true', 'love', 'story.', 'be', 'sure', 'to', 'have', 'a', 'few', 'hankies', 'ready', 'before', 'you', 'watch', 'this', 'movie---you', 'will', 'need', 'them.', 'extraordinary.'], 'label': '1'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTNAy--atkYU","executionInfo":{"elapsed":3915,"status":"ok","timestamp":1616656623059,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"f62b019f-0622-4849-ff6e-f5e5f683505d"},"source":["print(train_data.fields.items())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dict_items([('text', <torchtext.legacy.data.field.Field object at 0x7f2a5bf1cf50>), ('label', <torchtext.legacy.data.field.LabelField object at 0x7f2a5bf1cfd0>)])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qXLS9KfLt4eG"},"source":["## 4. Create Vocabulary"]},{"cell_type":"markdown","metadata":{"id":"3FBK5KiNv5jr"},"source":["- torchtext에서 제공하는 pretrained language model이 있다. 그 중 glove를 사용한다. \n","- unk_init은 unk 토큰을 어떻게 initialize할지를 나타낸다. "]},{"cell_type":"code","metadata":{"id":"pcRrZMAItq6-"},"source":["TEXT.build_vocab(train_data,\n","                min_freq=10, \n","                max_size=1000)\n","                 #  vectors = \"glove.6B.100d\",)\n","\n","LABEL.build_vocab(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-D5vce7t1rc","executionInfo":{"elapsed":1696,"status":"ok","timestamp":1616656630104,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"eb5e3091-6cf1-4335-f47d-e1eb570c9c9b"},"source":["print(len(TEXT.vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1002\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7IkkZqk-uAFW","executionInfo":{"elapsed":1263,"status":"ok","timestamp":1616656630105,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"d7c265c0-4539-47df-8f00-f94514545924"},"source":["print(TEXT.vocab.stoi)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f2a2e46afd0>>, {'<unk>': 0, '<pad>': 1, 'the': 2, 'a': 3, 'and': 4, 'of': 5, 'to': 6, 'is': 7, 'in': 8, 'i': 9, 'this': 10, 'it': 11, 'that': 12, '/><br': 13, 'was': 14, 'as': 15, 'for': 16, 'with': 17, 'but': 18, 'on': 19, 'movie': 20, 'his': 21, 'are': 22, 'not': 23, 'you': 24, 'film': 25, 'have': 26, 'he': 27, 'be': 28, 'at': 29, 'one': 30, 'by': 31, 'an': 32, 'they': 33, 'from': 34, 'all': 35, 'who': 36, 'like': 37, 'so': 38, 'just': 39, 'or': 40, 'has': 41, 'about': 42, 'her': 43, \"it's\": 44, 'if': 45, 'some': 46, 'out': 47, 'what': 48, 'when': 49, 'very': 50, 'there': 51, 'more': 52, 'would': 53, 'even': 54, 'my': 55, 'good': 56, 'she': 57, 'only': 58, 'no': 59, 'their': 60, 'really': 61, 'had': 62, 'up': 63, 'can': 64, 'which': 65, 'see': 66, 'were': 67, 'than': 68, '-': 69, 'we': 70, 'been': 71, 'get': 72, 'into': 73, 'will': 74, 'much': 75, 'because': 76, 'story': 77, 'most': 78, 'how': 79, 'other': 80, 'do': 81, 'me': 82, \"don't\": 83, 'its': 84, 'time': 85, 'great': 86, 'first': 87, 'also': 88, 'make': 89, 'people': 90, 'any': 91, 'could': 92, '/>the': 93, 'after': 94, 'made': 95, 'then': 96, 'bad': 97, 'think': 98, 'many': 99, 'him': 100, 'being': 101, 'never': 102, 'two': 103, '<br': 104, 'where': 105, 'little': 106, 'too': 107, 'watch': 108, 'well': 109, 'way': 110, 'your': 111, 'it.': 112, 'did': 113, 'them': 114, 'best': 115, 'love': 116, 'movie.': 117, 'know': 118, 'does': 119, 'characters': 120, 'these': 121, 'seen': 122, 'movies': 123, 'ever': 124, 'character': 125, 'over': 126, 'still': 127, 'films': 128, 'should': 129, 'show': 130, 'such': 131, 'plot': 132, 'acting': 133, 'better': 134, 'those': 135, 'while': 136, 'film.': 137, 'off': 138, 'something': 139, 'say': 140, 'go': 141, 'why': 142, 'through': 143, \"doesn't\": 144, \"didn't\": 145, 'makes': 146, \"i'm\": 147, 'watching': 148, 'film,': 149, 'scene': 150, 'find': 151, 'back': 152, 'real': 153, 'movie,': 154, 'every': 155, 'actually': 156, 'few': 157, 'scenes': 158, 'nothing': 159, 'going': 160, 'life': 161, 'man': 162, '/>i': 163, 'same': 164, 'look': 165, 'another': 166, 'new': 167, 'thing': 168, 'lot': 169, 'quite': 170, '&': 171, 'end': 172, 'want': 173, 'seems': 174, 'pretty': 175, 'old': 176, \"can't\": 177, 'before': 178, 'got': 179, 'part': 180, 'take': 181, 'years': 182, 'give': 183, 'may': 184, 'actors': 185, 'both': 186, \"i've\": 187, 'between': 188, \"that's\": 189, 'young': 190, 'without': 191, 'us': 192, 'big': 193, 'it,': 194, 'things': 195, 'saw': 196, 'thought': 197, 'now': 198, 'almost': 199, 'though': 200, 'director': 201, 'must': 202, 'around': 203, 'own': 204, 'gets': 205, \"isn't\": 206, 'come': 207, 'work': 208, 'always': 209, 'here': 210, '\"the': 211, 'whole': 212, 'horror': 213, 'down': 214, 'might': 215, \"there's\": 216, 'am': 217, 'cast': 218, 'long': 219, 'enough': 220, 'probably': 221, 'since': 222, 'least': 223, \"he's\": 224, 'bit': 225, 'last': 226, 'feel': 227, 'far': 228, 'each': 229, 'funny': 230, 'fact': 231, 'found': 232, 'kind': 233, 'rather': 234, 'our': 235, 'original': 236, 'world': 237, 'interesting': 238, 'making': 239, 'worst': 240, 'guy': 241, 'anything': 242, 'done': 243, 'having': 244, 'comes': 245, 'trying': 246, 'believe': 247, 'however,': 248, 'action': 249, 'right': 250, 'point': 251, 'anyone': 252, 'put': 253, 'music': 254, '/>this': 255, 'goes': 256, 'main': 257, 'played': 258, 'worth': 259, 'hard': 260, 'watched': 261, 'yet': 262, 'role': 263, 'looks': 264, 'especially': 265, \"wasn't\": 266, 'plays': 267, 'looking': 268, 'tv': 269, 'family': 270, 'series': 271, 'three': 272, 'takes': 273, 'script': 274, 'seem': 275, 'shows': 276, 'someone': 277, 'minutes': 278, 'sure': 279, 'away': 280, 'during': 281, 'everything': 282, 'different': 283, 'performance': 284, 'comedy': 285, 'set': 286, 'maybe': 287, 'times': 288, 'although': 289, 'time.': 290, 'woman': 291, 'left': 292, 'once': 293, 'seeing': 294, 'fun': 295, 'simply': 296, 'american': 297, \"you're\": 298, 'play': 299, 'everyone': 300, 'girl': 301, 'special': 302, 'true': 303, 'completely': 304, 'again': 305, 'used': 306, 'john': 307, 'read': 308, 'well,': 309, '--': 310, 'need': 311, 'reason': 312, 'dvd': 313, 'until': 314, 'high': 315, 'idea': 316, 'use': 317, 'given': 318, 'sense': 319, 'beautiful': 320, 'money': 321, 'place': 322, 'truly': 323, 'black': 324, 'try': 325, 'version': 326, 'came': 327, 'let': 328, 'help': 329, 'nice': 330, 'less': 331, 'recommend': 332, 'poor': 333, 'getting': 334, 'job': 335, 'keep': 336, 'excellent': 337, 'along': 338, 'ending': 339, 'shot': 340, 'actor': 341, 'said': 342, 'rest': 343, 'half': 344, 'couple': 345, 'effects': 346, 'full': 347, 'enjoy': 348, 'tell': 349, \"couldn't\": 350, 'instead': 351, 'second': 352, '(and': 353, 'went': 354, 'gives': 355, 'next': 356, 'day': 357, 'fan': 358, 'audience': 359, 'definitely': 360, 'absolutely': 361, 'short': 362, 'himself': 363, 'become': 364, 'book': 365, 'entire': 366, 'playing': 367, 'together': 368, 'understand': 369, 'war': 370, 'remember': 371, 'early': 372, 'screen': 373, 'all,': 374, 'later': 375, 'hollywood': 376, '2': 377, 'doing': 378, 'liked': 379, '(the': 380, 'several': 381, 'start': 382, 'human': 383, 'loved': 384, 'kids': 385, 'wife': 386, 'felt': 387, 'certainly': 388, 'piece': 389, 'supposed': 390, 'small': 391, 'totally': 392, 'against': 393, 'star': 394, 'perhaps': 395, 'year': 396, 'sort': 397, 'becomes': 398, 'men': 399, 'often': 400, 'is,': 401, 'time,': 402, 'seemed': 403, 'based': 404, 'wonderful': 405, 'wanted': 406, \"you'll\": 407, 'night': 408, 'classic': 409, 'home': 410, 'camera': 411, 'waste': 412, 'else': 413, 'production': 414, 'able': 415, '10': 416, 'hope': 417, '.': 418, 'video': 419, 'friends': 420, '\\x96': 421, 'course': 422, 'line': 423, 'top': 424, \"she's\": 425, 'women': 426, 'that,': 427, 'final': 428, 'live': 429, 'called': 430, 'them.': 431, 'sound': 432, 'house': 433, 'name': 434, 'performances': 435, 'school': 436, 'death': 437, 'mind': 438, 'person': 439, 'lost': 440, 'one.': 441, 'turn': 442, 'care': 443, 'father': 444, 'wants': 445, 'tries': 446, 'story,': 447, 'stupid': 448, 'sex': 449, 'gave': 450, \"won't\": 451, 'under': 452, 'already': 453, 'perfect': 454, 'this,': 455, 'lead': 456, \"they're\": 457, 'low': 458, 'either': 459, 'despite': 460, \"i'd\": 461, 'finally': 462, 'dead': 463, 'turns': 464, 'starts': 465, 'moments': 466, 'this.': 467, 'episode': 468, 'took': 469, 'enjoyed': 470, 'him.': 471, 'mean': 472, 'written': 473, 'lines': 474, 'face': 475, 'problem': 476, 'guess': 477, 'budget': 478, 'title': 479, 'me,': 480, 'me.': 481, 'behind': 482, 'highly': 483, 'kill': 484, 'and,': 485, 'terrible': 486, 'head': 487, 'cannot': 488, 'guys': 489, 'fine': 490, 'michael': 491, 'story.': 492, 'friend': 493, 'white': 494, 'favorite': 495, 'well.': 496, 'looked': 497, 'beginning': 498, 'stars': 499, 'itself': 500, \"wouldn't\": 501, 'boy': 502, '/>it': 503, 'run': 504, 'extremely': 505, 'others': 506, 'quality': 507, 'works': 508, 'all.': 509, 'evil': 510, 'boring': 511, 'lack': 512, 'fans': 513, 'dialogue': 514, 'laugh': 515, 'case': 516, 'sometimes': 517, 'mr.': 518, 'mother': 519, '/>in': 520, 'lives': 521, 'taken': 522, 'feeling': 523, 'good.': 524, 'dark': 525, 'save': 526, 'wonder': 527, 'attempt': 528, 'heard': 529, 'good,': 530, 'style': 531, 'directed': 532, 'obviously': 533, 'expect': 534, 'wrong': 535, 'leave': 536, 'decent': 537, 'throughout': 538, 'picture': 539, 'writing': 540, 'late': 541, 'shown': 542, 'particularly': 543, 'group': 544, '3': 545, 'life.': 546, \"film's\": 547, 'entertaining': 548, 'car': 549, 'complete': 550, 'across': 551, 'soon': 552, 'told': 553, 'exactly': 554, 'coming': 555, 'stop': 556, 'viewer': 557, 'usually': 558, 'fight': 559, 'killed': 560, 'police': 561, 'children': 562, 'movies,': 563, 'strong': 564, 'game': 565, 'killer': 566, 'worse': 567, 'parts': 568, 'movies.': 569, 'type': 570, 'films,': 571, 'huge': 572, 'says': 573, 'thinking': 574, 'side': 575, 'course,': 576, 'number': 577, 'except': 578, 'finds': 579, '/>if': 580, 'close': 581, 'opening': 582, 'bad.': 583, 'knew': 584, 'whose': 585, 'due': 586, 'amazing': 587, 'taking': 588, 'known': 589, 'myself': 590, ',': 591, 'living': 592, 'major': 593, 'awful': 594, 'running': 595, 'acting,': 596, 'local': 597, 'wish': 598, 'turned': 599, 'somewhat': 600, 'way,': 601, 'beyond': 602, 'matter': 603, 'call': 604, 'again,': 605, 'past': 606, 'act': 607, 'female': 608, 'happens': 609, 'tells': 610, 'obvious': 611, 'brilliant': 612, 'that.': 613, 'horrible': 614, 'none': 615, 'direction': 616, 'here,': 617, 'hour': 618, 'it.<br': 619, 'robert': 620, 'serious': 621, 'hit': 622, 'cinema': 623, 'fact,': 624, 'girls': 625, 'including': 626, 'son': 627, 'town': 628, \"i'll\": 629, 'james': 630, 'saying': 631, 'bad,': 632, 'kid': 633, 'involved': 634, 'started': 635, 'characters,': 636, 'david': 637, 'humor': 638, 'mostly': 639, 'art': 640, 'out.': 641, 'order': 642, 'single': 643, 'relationship': 644, 'cut': 645, 'bring': 646, 'knows': 647, '/>there': 648, 'giving': 649, 'upon': 650, \"aren't\": 651, 'simple': 652, 'end,': 653, 'seen.': 654, 'actress': 655, 'history': 656, 'whether': 657, 'drama': 658, 'four': 659, 'supporting': 660, 'chance': 661, 'falls': 662, 'themselves': 663, 'lots': 664, 'voice': 665, 'british': 666, 'english': 667, 'talking': 668, 'him,': 669, 'clearly': 670, \"haven't\": 671, 'child': 672, 'eyes': 673, 'here.': 674, 'ends': 675, 'easily': 676, 'however': 677, '(i': 678, 'appears': 679, 'fall': 680, 'released': 681, 'stories': 682, 'stuff': 683, '/>but': 684, 'important': 685, 'modern': 686, 'score': 687, 'stay': 688, 'end.': 689, 'change': 690, 'films.': 691, 'certain': 692, 'happened': 693, 'hours': 694, 'days': 695, 'example': 696, 'one,': 697, 'using': 698, 'feels': 699, 'heart': 700, 'moment': 701, 'add': 702, 'musical': 703, 'tried': 704, 'similar': 705, 'among': 706, 'five': 707, 'but,': 708, 'strange': 709, 'named': 710, 'nearly': 711, 'needs': 712, 'within': 713, 'is.': 714, 'again.': 715, 'french': 716, 'hate': 717, 'yes,': 718, 'buy': 719, 'mention': 720, 'city': 721, '/>': 722, 'basically': 723, 'comic': 724, 'greatest': 725, 'movie.<br': 726, 'kept': 727, 'working': 728, 'brought': 729, 'miss': 730, 'actual': 731, 'shots': 732, 'apparently': 733, 'plot,': 734, '/>and': 735, 'blood': 736, 'jokes': 737, 'showing': 738, 'film.<br': 739, 'song': 740, '/>a': 741, 'bunch': 742, 'her.': 743, 'jack': 744, 'slow': 745, 'way.': 746, 'life,': 747, 'near': 748, 'usual': 749, 'yourself': 750, 'happy': 751, 'daughter': 752, 'middle': 753, 'cheap': 754, 'sad': 755, 'surprised': 756, \"what's\": 757, 'typical': 758, 'ten': 759, 'documentary': 760, 'alone': 761, 'body': 762, 'interest': 763, 'talk': 764, 'view': 765, 'overall': 766, 'decided': 767, 'on.': 768, 'sit': 769, 'famous': 770, 'learn': 771, 'imagine': 772, 'silly': 773, 'murder': 774, 'romantic': 775, 'filmed': 776, '/>as': 777, 'easy': 778, 'so,': 779, 'events': 780, '(which': 781, 'clear': 782, 'reality': 783, 'oh': 784, 'gone': 785, 'became': 786, 'difficult': 787, 'experience': 788, 'happen': 789, 'hear': 790, 'power': 791, 'cool': 792, 'flick': 793, '5': 794, 'them,': 795, 'funny.': 796, 'also,': 797, 'hell': 798, '(as': 799, \"you've\": 800, 'attention': 801, 'talent': 802, 'too.': 803, 'george': 804, 'meets': 805, 'please': 806, 'cinematography': 807, 'television': 808, 'leaves': 809, 'light': 810, 'rent': 811, 'word': 812, 'violence': 813, 'peter': 814, 'begins': 815, \"/>it's\": 816, 'keeps': 817, 'ridiculous': 818, 'above': 819, '(or': 820, 'age': 821, 'sets': 822, 'husband': 823, 'means': 824, 'funny,': 825, 'character,': 826, 'richard': 827, 'annoying': 828, 'emotional': 829, 'figure': 830, 'towards': 831, 'doubt': 832, 'eventually': 833, 'deal': 834, 'somehow': 835, 'characters.': 836, 'possibly': 837, 'roles': 838, 'sexual': 839, 'sequence': 840, 'brother': 841, 'genre': 842, 'move': 843, 'moving': 844, \"who's\": 845, 'possible': 846, 'straight': 847, 'hilarious': 848, 'killing': 849, 'poorly': 850, 'hero': 851, 'gore': 852, 'god': 853, 'nor': 854, 'feature': 855, 'incredibly': 856, 'team': 857, '1': 858, 'previous': 859, 'show.': 860, 'check': 861, 'songs': 862, 'ones': 863, 'out,': 864, 'career': 865, 'problems': 866, 'elements': 867, 'on,': 868, 'leads': 869, 'say,': 870, 'words': 871, 'comments': 872, 'country': 873, 'leading': 874, 'begin': 875, '4': 876, 'review': 877, 'hand': 878, 'watch.': 879, 'write': 880, 'forget': 881, 'level': 882, 'tom': 883, 'various': 884, 'de': 885, 'forced': 886, 'subject': 887, 'paul': 888, 'room': 889, 'though,': 890, 'theme': 891, 'whom': 892, 'dr.': 893, 'fairly': 894, 'novel': 895, 'unfortunately': 896, 'red': 897, 'rock': 898, 'now,': 899, 'total': 900, 'reading': 901, 'enjoyable': 902, 'episodes': 903, 'scary': 904, 'spent': 905, 'avoid': 906, 'effort': 907, 'needed': 908, 'brings': 909, 'personal': 910, 'tale': 911, 'realize': 912, 'japanese': 913, '...': 914, 'interested': 915, 'meet': 916, 'stand': 917, 'front': 918, 'sounds': 919, 'monster': 920, 'political': 921, 'follow': 922, 'male': 923, 'plenty': 924, 'scenes,': 925, 'you.': 926, 'form': 927, 'pay': 928, 'up.': 929, 'appear': 930, 'third': 931, 'writer': 932, 'hardly': 933, 'crap': 934, \"let's\": 935, 'there.': 936, 'unless': 937, 'features': 938, 'parents': 939, 'plain': 940, 'reviews': 941, 'message': 942, 'general': 943, 'open': 944, 'create': 945, 'oscar': 946, 'points': 947, 'man,': 948, 'worked': 949, 'expected': 950, 'ask': 951, 'better.': 952, 'meant': 953, 'expecting': 954, 'manages': 955, 'soundtrack': 956, 'viewers': 957, 'unfortunately,': 958, 'up,': 959, 'weak': 960, 'dialog': 961, 'lady': 962, 'copy': 963, 'deep': 964, 'fast': 965, 'king': 966, '20': 967, 'particular': 968, 'work.': 969, 'scene,': 970, 'space': 971, 'herself': 972, 'average': 973, 'caught': 974, '(a': 975, 'attempts': 976, 'premise': 977, 'times,': 978, 'minute': 979, 'then,': 980, 'dramatic': 981, 'inside': 982, 'pure': 983, 'class': 984, 'sister': 985, 'amount': 986, 'older': 987, 'crime': 988, 'footage': 989, 'her,': 990, 'whatever': 991, 'future': 992, 'decides': 993, 'earth': 994, 'uses': 995, 'storyline': 996, 'portrayed': 997, 'forward': 998, 'western': 999, 'animation': 1000, 'theater': 1001})\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eGFw6_4SuL25"},"source":["## 5. DataLoader for TorchText"]},{"cell_type":"code","metadata":{"id":"NOsvwfIbuPbg"},"source":["BATCH_SIZE = 16\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True,\n","    sort_key = lambda x: len(x.text),\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2nA8LmIuUpf","executionInfo":{"elapsed":6227,"status":"ok","timestamp":1616656661804,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"e27a25a4-26cb-4049-9ac0-e17274362d02"},"source":["batch = next(iter(train_iterator))\n","print(type(batch))\n","print(batch)\n","print(batch.text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'torchtext.legacy.data.batch.Batch'>\n","\n","[torchtext.legacy.data.batch.Batch of size 16]\n","\t[.text]:('[torch.cuda.LongTensor of size 16x880 (GPU 0)]', '[torch.cuda.LongTensor of size 16 (GPU 0)]')\n","\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n","(tensor([[  8,   2, 582,  ...,  43,   0,   0],\n","        [ 10,  25,   0,  ...,   1,   1,   1],\n","        [ 10,  14, 723,  ...,   1,   1,   1],\n","        ...,\n","        [ 78,  53,   0,  ...,   1,   1,   1],\n","        [ 49,   3,   0,  ...,   1,   1,   1],\n","        [172,   5, 695,  ...,   1,   1,   1]], device='cuda:0'), tensor([880, 853, 843, 837, 815, 807, 799, 784, 779, 777, 766, 761, 755, 748,\n","        746, 746], device='cuda:0'))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6cAAIcV8wUD6"},"source":["## 6. Model Architecture"]},{"cell_type":"code","metadata":{"id":"VOd2zxXHwTbn"},"source":["class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text, text_lengths):\n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(text))        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        # pack sequence\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), batch_first=True)        \n","        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n","        \n","        #unpack sequence\n","        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h3lITaN4ytHh"},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8D1hnDbyw6c","executionInfo":{"elapsed":722,"status":"ok","timestamp":1616656661806,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"c14bd7d1-6bda-493f-b917-1432efe85e20"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 2,410,857 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9P8BIvqPyywy"},"source":["# pretrained_embeddings = TEXT.vocab.vectors\n","\n","# print(pretrained_embeddings.shape)\n","# model.embedding.weight.data.copy_(pretrained_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWRSj2Qby0L-","executionInfo":{"elapsed":566,"status":"ok","timestamp":1616656665579,"user":{"displayName":"stp L","photoUrl":"","userId":"16125751050798431485"},"user_tz":-540},"outputId":"49f18ad7-0e3d-4a93-8a08-95050f7193f0"},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.5132, -0.0830, -0.4850,  ..., -0.3438,  0.5329,  1.7657],\n","        ...,\n","        [-0.0773,  0.3457, -0.3664,  ..., -0.6953, -0.5017, -1.1514],\n","        [-0.6136, -1.0967, -0.1012,  ...,  0.0047,  1.4294, -1.8340],\n","        [-0.1804, -0.1241, -1.0578,  ...,  0.3118,  0.0476, -1.8583]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t454yuOgy7Pz"},"source":["## 7. Train & Evaluate Model"]},{"cell_type":"code","metadata":{"id":"4I0Bzq7uy5gn"},"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.BCEWithLogitsLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sRgkw061zBpV"},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTuFdomgzDsw"},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","        text, text_lengths = batch.text\n","        predictions = model(text, text_lengths).squeeze(1)\n","        loss = criterion(predictions, batch.label)\n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hloKVOuHzFay"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","\n","            text, text_lengths = batch.text\n","            \n","            predictions = model(text, text_lengths).squeeze(1)\n","            \n","            loss = criterion(predictions, batch.label)\n","            \n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUnjs17zzHRE"},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"pWZpWs8MzIxO","outputId":"7479d531-752f-4fd8-b62a-35811ce901bb"},"source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), os.path.join(path, 'rnn-imdb-sentiment.pt'))\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 2m 17s\n","\tTrain Loss: 0.683 | Train Acc: 55.24%\n","\t Val. Loss: 0.691 |  Val. Acc: 53.02%\n","Epoch: 02 | Epoch Time: 2m 17s\n","\tTrain Loss: 0.637 | Train Acc: 62.06%\n","\t Val. Loss: 0.599 |  Val. Acc: 63.83%\n","Epoch: 03 | Epoch Time: 2m 17s\n","\tTrain Loss: 0.478 | Train Acc: 77.03%\n","\t Val. Loss: 0.430 |  Val. Acc: 80.08%\n","Epoch: 04 | Epoch Time: 2m 17s\n","\tTrain Loss: 0.400 | Train Acc: 81.77%\n","\t Val. Loss: 0.375 |  Val. Acc: 83.58%\n","Epoch: 05 | Epoch Time: 2m 17s\n","\tTrain Loss: 0.367 | Train Acc: 83.70%\n","\t Val. Loss: 0.366 |  Val. Acc: 84.46%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4k3top_azKo0"},"source":["model.load_state_dict(torch.load(os.path.join(path, 'rnn-imdb-sentiment.pt')))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbnXghEs4_zQ"},"source":[""],"execution_count":null,"outputs":[]}]}